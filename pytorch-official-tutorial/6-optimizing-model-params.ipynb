{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6558f836-0465-4354-961f-123f84dbfbe4",
   "metadata": {},
   "source": [
    "# Optimizing model parameters\n",
    "\n",
    "이제 우리는 학습할 모델과 데이터를 다 갖고 있다,  \n",
    "우리의 데이터 위에서 파라미터들을 최적화하여 모델을 검증하고 테스트한다.  \n",
    "모델을 학습하는 것은 iterative한 과정이다.  \n",
    "매 epoch마다 output에 대한 예측을 생성하고,  \n",
    "해당 예측의 error를 계산한다, 이것이 바로 loss이고,  \n",
    "해당 각 파라미터들에 관한 error의 도함수를 수집하고,  \n",
    "gradient descent(경사 하강법)을 사용해 파라미터들을 최적화한다.  \n",
    "\n",
    "-> 더 자세히 이 과정을 알고 싶으면 다음 링크를 참고하라  \n",
    "(홈페이지 가서 보삼)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf12c6fa-2f2a-4255-aa24-5b7335866e86",
   "metadata": {},
   "source": [
    "## Prerequisite Code\n",
    "\n",
    "앞 챕터에서 생성한 코드를 가져오자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5200177d-5070-45a7-ba50-dccd2c7dd7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd7be1-fec0-4330-ae9c-dca674a48be2",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "하이퍼파라미터들은 모델 최적화 과정을 제어하기 위해, 조절할 수 있는 파라미터들이다.  \n",
    "다른 하이퍼파라미터 값들은 모델 학습과 convergence 비율에 영향을 줄 수 있다.  \n",
    "(hyperparameter tuning을 참고할 것)\n",
    "\n",
    "학습을 위한 다음과 같은 하이퍼파라미터들이 있다.\n",
    "\n",
    "- Number of Epochs\n",
    "  - 데이터셋 반복 수\n",
    "- Batch size\n",
    "  - 파라미터 업데이트 전 신경망으로 전파할 데이터 샘플들의 수\n",
    "- Learning rate\n",
    "  - 매 batch/epoch 마다 얼마나 모델 파라미터들을 업데이트 할 것인가.\n",
    "  - 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9007ab7-afed-45e0-b4dd-02b0da7752ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594d2a8-8dd7-43db-8c8b-f49fca633361",
   "metadata": {},
   "source": [
    "## Optimization Loop\n",
    "우리의 하이퍼파라미터들을 설정하고,  \n",
    "우리는 optimization loop와 함께 우리의 모델을 학습하고 최적화할 수 있다.  \n",
    "optimization loop에서의 매 iteration을 epoch라고 부른다.\n",
    "\n",
    "매 epoch는 2가지 주요 부분으로 구성되어 있다.\n",
    "\n",
    "- The Train loop\n",
    "  - 학습용 데이터셋으로 반복하고, 최적의 파라피터로 수렴하도록 시도한다.\n",
    "- The Validation/Test Loop\n",
    "  - 테스트용 데이터셋으로 반복하고, 모델 퍼포먼스 개선이 있었는지 체크한다\n",
    "  \n",
    "학습 루프에서 사용되는 개념들을 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4db970-e7e7-4b4d-9d55-ea31193e9bf4",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "아직 학습되지 않은 신경망은 틀린 답을 내기 쉽다.  \n",
    "Loss function은 target value와 얻어진 결과의 다름의 정도를 측정하고,  \n",
    "그것이 loss function이고 우리가 학습 동안 최소화 해야할 대상이다.\n",
    "\n",
    "loss를 계산하기 위해 우리는 주어진 데이터 샘플을 사용해 prediction을 만들고  \n",
    "그것을 true 데이터 라벨 값과 비교해야한다.\n",
    "\n",
    "자주 쓰이는 loss functions는 다음과 같다.\n",
    "\n",
    "- `nn.MSELoss`(Mean Square Error)\n",
    "  - regression tasks(회귀)에 사용\n",
    "- `nn.NLLLoss`(Negative Log Likelihood)\n",
    "  - 분류 문제에 자주 사용\n",
    "- `nn.CrossEntropyLoss`\n",
    "  - `nn.LogSoftmax`와 `nn.NLLLoss`를 합쳐서 만든 것\n",
    "  \n",
    "우리는 `nn.CrossEntropyLoss`를 예제에서 사용한다.  \n",
    "이를 통해 logits를 정규화하고, prediction error를 계산한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f3eead-798c-4085-b723-f1cd4b2732ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c03203-2197-4ded-8c03-ace1159ba1c3",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Optimization은 매 학습 단계에서 모델 에러를 줄이도록 모델 파라미터들을 적응시키는 과정이다.  \n",
    "Optimization algorithms는 어떻게 이러한 과정들을 수행하는지 정의한다.  \n",
    "(예를 들어 우리는 SGD, Stochastic Gradient Descent를 사용한다)\n",
    "\n",
    "모든 optimization logic은 `optimizer` 객체로 캡슐화된다.  \n",
    "여기서 우리는 SGD optimizer를 사용한다.\n",
    "\n",
    "PyTorch에서 사용가능한 옵티마이저들은 다음과 같다.  \n",
    "이들은 다양한 모델과 데이터의 경우에서 잘 작동한다.\n",
    "\n",
    "- ADAM\n",
    "- RMSProp\n",
    "\n",
    "우리는 학습되어야 할 모델의 파라미터들을 등록하고,  \n",
    "하이퍼파라미터인 학습률을 전달하여 옵티마이저를 초기화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "409c817a-0e7e-40b3-9b27-e4f01739cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801bdd7-c227-47f3-bb99-7bb843d40adb",
   "metadata": {},
   "source": [
    "Training loop안에서,  \n",
    "3가지 단계가 수행된다.\n",
    "\n",
    "- 모델 파라미터들의 기울기를 초기화하기 위해 `optimizer.zero_grad()`를 호출한다.\n",
    "  - 기울기들은 기본적으로 더해진다.\n",
    "  - 그래서 double-counting을 방지하기 위해, 우리는 명백히 매 iteration마다 이를 호출한다.\n",
    "- `loss.backwards()`를 호출하면서 prediction loss를 역전파한다.\n",
    "  - PyTorch는 loss 기울기를 매 파라미터마다 저장한다\n",
    "- 우리의 기울기 값을 다 저장하고 난 후, 우리는 `optimizer.step()`을 호출한다\n",
    "  - backward 단계에서 수집된 기울기 값들로 파라미터들을 업데이트한다.\n",
    "  \n",
    "(내 이해)\n",
    "- `optimizer.zero_grad()`: 이전에 저장된 값 초기화\n",
    "- `loss.backwards()`: 각 파라미터마다 loss 값으로 기울기 업데이트\n",
    "- `optimizer.step()`: 업데이트된 기울기 값들로 파라미터 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0850b-17b9-46ed-afe7-144fd0736e57",
   "metadata": {},
   "source": [
    "## Full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df18d41e-b3b6-4e61-ac40-cce66015f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27234a59-8560-43c9-8df9-f4105e92979a",
   "metadata": {},
   "source": [
    "우리는 loss function과 optimizer을 초기화하고,  \n",
    "이를 `train_loop`와 `test_loop`에 전달한다.\n",
    "\n",
    "모델의 개선된 퍼포먼스를 추적하기 위한 epoch의 수를 늘리는 것을 편하게 생각해라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22690720-df3a-4e81-be59-d112817216f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------\n",
      "loss: 2.304460 [    0/60000]\n",
      "loss: 2.292843 [ 6400/60000]\n",
      "loss: 2.288857 [12800/60000]\n",
      "loss: 2.286781 [19200/60000]\n",
      "loss: 2.257131 [25600/60000]\n",
      "loss: 2.251544 [32000/60000]\n",
      "loss: 2.260139 [38400/60000]\n",
      "loss: 2.237183 [44800/60000]\n",
      "loss: 2.245186 [51200/60000]\n",
      "loss: 2.213186 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 0.034858 \n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 2.235236 [    0/60000]\n",
      "loss: 2.228448 [ 6400/60000]\n",
      "loss: 2.208048 [12800/60000]\n",
      "loss: 2.222889 [19200/60000]\n",
      "loss: 2.141331 [25600/60000]\n",
      "loss: 2.133320 [32000/60000]\n",
      "loss: 2.167587 [38400/60000]\n",
      "loss: 2.112585 [44800/60000]\n",
      "loss: 2.153861 [51200/60000]\n",
      "loss: 2.076620 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.5%, Avg loss: 0.032915 \n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 2.133714 [    0/60000]\n",
      "loss: 2.127917 [ 6400/60000]\n",
      "loss: 2.080720 [12800/60000]\n",
      "loss: 2.120383 [19200/60000]\n",
      "loss: 1.951496 [25600/60000]\n",
      "loss: 1.950913 [32000/60000]\n",
      "loss: 2.025215 [38400/60000]\n",
      "loss: 1.926187 [44800/60000]\n",
      "loss: 2.022485 [51200/60000]\n",
      "loss: 1.892095 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.8%, Avg loss: 0.030269 \n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 1.996044 [    0/60000]\n",
      "loss: 2.001132 [ 6400/60000]\n",
      "loss: 1.926085 [12800/60000]\n",
      "loss: 1.999170 [19200/60000]\n",
      "loss: 1.732815 [25600/60000]\n",
      "loss: 1.775085 [32000/60000]\n",
      "loss: 1.882335 [38400/60000]\n",
      "loss: 1.756176 [44800/60000]\n",
      "loss: 1.896763 [51200/60000]\n",
      "loss: 1.746442 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 42.7%, Avg loss: 0.028067 \n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "loss: 1.870488 [    0/60000]\n",
      "loss: 1.898296 [ 6400/60000]\n",
      "loss: 1.806498 [12800/60000]\n",
      "loss: 1.913319 [19200/60000]\n",
      "loss: 1.576857 [25600/60000]\n",
      "loss: 1.659664 [32000/60000]\n",
      "loss: 1.785480 [38400/60000]\n",
      "loss: 1.642178 [44800/60000]\n",
      "loss: 1.796046 [51200/60000]\n",
      "loss: 1.657699 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 0.026560 \n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "loss: 1.769786 [    0/60000]\n",
      "loss: 1.817987 [ 6400/60000]\n",
      "loss: 1.711379 [12800/60000]\n",
      "loss: 1.852493 [19200/60000]\n",
      "loss: 1.472894 [25600/60000]\n",
      "loss: 1.580887 [32000/60000]\n",
      "loss: 1.719036 [38400/60000]\n",
      "loss: 1.562859 [44800/60000]\n",
      "loss: 1.717752 [51200/60000]\n",
      "loss: 1.598077 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 0.025461 \n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "loss: 1.691081 [    0/60000]\n",
      "loss: 1.753949 [ 6400/60000]\n",
      "loss: 1.632378 [12800/60000]\n",
      "loss: 1.801748 [19200/60000]\n",
      "loss: 1.402344 [25600/60000]\n",
      "loss: 1.522631 [32000/60000]\n",
      "loss: 1.670776 [38400/60000]\n",
      "loss: 1.503806 [44800/60000]\n",
      "loss: 1.657832 [51200/60000]\n",
      "loss: 1.553306 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.4%, Avg loss: 0.024620 \n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "loss: 1.629601 [    0/60000]\n",
      "loss: 1.701612 [ 6400/60000]\n",
      "loss: 1.565820 [12800/60000]\n",
      "loss: 1.759308 [19200/60000]\n",
      "loss: 1.353430 [25600/60000]\n",
      "loss: 1.477583 [32000/60000]\n",
      "loss: 1.634848 [38400/60000]\n",
      "loss: 1.456854 [44800/60000]\n",
      "loss: 1.611461 [51200/60000]\n",
      "loss: 1.518140 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.1%, Avg loss: 0.023957 \n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "loss: 1.580570 [    0/60000]\n",
      "loss: 1.658056 [ 6400/60000]\n",
      "loss: 1.509524 [12800/60000]\n",
      "loss: 1.723114 [19200/60000]\n",
      "loss: 1.317948 [25600/60000]\n",
      "loss: 1.441537 [32000/60000]\n",
      "loss: 1.605957 [38400/60000]\n",
      "loss: 1.419392 [44800/60000]\n",
      "loss: 1.573933 [51200/60000]\n",
      "loss: 1.489628 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 0.023420 \n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "loss: 1.538722 [    0/60000]\n",
      "loss: 1.621593 [ 6400/60000]\n",
      "loss: 1.462088 [12800/60000]\n",
      "loss: 1.693931 [19200/60000]\n",
      "loss: 1.292069 [25600/60000]\n",
      "loss: 1.411633 [32000/60000]\n",
      "loss: 1.580863 [38400/60000]\n",
      "loss: 1.388419 [44800/60000]\n",
      "loss: 1.542825 [51200/60000]\n",
      "loss: 1.464484 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 0.022971 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb0441-a345-497e-a938-3562b6476aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
