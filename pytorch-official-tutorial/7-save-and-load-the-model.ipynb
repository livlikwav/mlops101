{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a74177e-aa86-4d43-83cf-d08e3746ae4a",
   "metadata": {},
   "source": [
    "# Save and load the model\n",
    "\n",
    "이 섹션에서 우리는 모델의 상태를 어떻게 저장할 것인가 살펴볼 것이다,  \n",
    "\n",
    "- saving\n",
    "- loading\n",
    "- running model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e387716c-5e50-4d02-827a-2182613070e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx as onnx\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4761f0-4203-463a-b70a-7c84cf2d9091",
   "metadata": {},
   "source": [
    "## Saving and Loading Model Weights\n",
    "\n",
    "PyTorch 모델들은 학습된 파라미터를 내부 상태 사전 자료형에 저장한다,  \n",
    "이는 `state_dict`로 불린다.  \n",
    "이는 `torch.save()` 메소드를 통해 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1b9670-bd08-4cfa-a87f-6c50444bdc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/livlikwav/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "4.4%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "10.3%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "15.9%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "23.5%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "38.5%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "46.1%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "54.7%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "62.2%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "70.4%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "79.4%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "88.0%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "95.9%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4662a9-4e53-41d8-8721-ab255a266848",
   "metadata": {},
   "source": [
    "model weights를 불러오기 위해서, 우리는 같은 모델 인스턴스를 먼저 생성할 필요가 있다,  \n",
    "그리고 파라미터들을 `load_state_dict()` 메소드를 사용해서 불러와야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6752e989-cda6-41fd-8d47-8c457cc76372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8778646-141c-494b-8d35-3a478038f329",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "inferencing(추론) 하기 전에 `model.eval()`을 꼭 호출해야한다.  \n",
    "이를 통해 dropout과 batch normalization layers를 evalution mode로 설정한다.  \n",
    "\n",
    "이거 호출안하면 일관되지 않는 inference 결과값들 나올꺼임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae68ab-c79c-4ad2-bee1-d56c0109f458",
   "metadata": {},
   "source": [
    "## Saving and Loading Models with Shapes\n",
    "\n",
    "model weights를 불러올 때,  \n",
    "우리는 모델 클래스를 먼저 초기화할 필요가 있다,  \n",
    "왜냐하면 그 클래스가 신경망의 구조를 정의하기 때문이다.\n",
    "\n",
    "우리는 모델의 구조까지 함께 저장하고 싶을 수 있다,  \n",
    "이 경우에 우리는 `model`을 저장하는 함수에 전달한다. (`model.state_dict()` 말고 `model` 그대로를)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3933f022-938b-447d-a0a5-610b2917332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e3880-bb5e-4cdf-96ef-d9e406841fd5",
   "metadata": {},
   "source": [
    "우리는 이 모델을 이렇게 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a3953fc-6a06-405f-bf78-e50ccfb8a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5af76d-578f-4162-bfde-024f07a154f8",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "이 접근 방법은 모델을 serializing(직렬화)할 때 Python의 pickle 모듈을 사용한다,  \n",
    "그러므로 모델을 불러올 때 실제 클래스 정의에 의존한다.\n",
    "\n",
    "(내 이해)불러온 인스턴스를 인터프리터에서 이해하려면, 해당 클래스가 먼저 정의되어 있어야 하나보다.  \n",
    "근데 그러면 결국 원본 소스코드도 같이 저장해야하네?  \n",
    "\n",
    "위에 예시는 dict로 저장해서 용량 적고, 모델 인스턴스 직접 생성해서 거기다가 붓는 느낌이면,  \n",
    "이건 인스턴스 그대로 떠서 저장하는데, 결국 클래스 정의는 있어야 하는 느낌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e945feec-6e44-43ce-b3e4-1c7ddcabf0f7",
   "metadata": {},
   "source": [
    "## Exporting model to ONNX\n",
    "\n",
    "파이토치는 native하게 ONNX 내보내기도 지원한다.  \n",
    "그러나, 파이토치의 실행 그래프의 동적인 구성으로, 내보내기 과정에서 실행 그래프를 순회할 필요가 있다.  \n",
    "이러한 이유로, 적절한 사이즈의 테스트 변수가 내보내기 함수에 전달되어야 한다.  \n",
    "(우리 예시에서는 알맞은 사이즈의 더비 0값 텐서를 생성한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5648f5b9-319c-4a39-bcaf-cfda115e8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.zeros((1, 3, 224, 224))\n",
    "onnx.export(model, input_image, 'model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c1577-3dc5-439f-be7e-806d45de73f6",
   "metadata": {},
   "source": [
    "ONNX 모델로 할 수 있는 것들이 많다.\n",
    "\n",
    "- 다른 플랫폼에서 인퍼런스 수행하기\n",
    "- 다른 프로그래밍 언어에서 인퍼런스 수행하기\n",
    "\n",
    "더 자세한 내용은 ONNX tutorial을 참고해라"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55f6d0-6728-4ae3-ab65-e8a4c52cf9c3",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "파이토치 초심자 튜토리얼을 완료했다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69decd5f-4fdd-46ec-93dd-2ebf8be81b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
